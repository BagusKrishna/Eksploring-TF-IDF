{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3f9e27",
   "metadata": {},
   "source": [
    "## READING DATA FROM LOCAL\n",
    "Ya disini cuma read data yang masih belum di labeling semua, jadi stelah read langsung drop yang null dulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3e0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully\n",
      "Jumlah baris invalid unik: 1924\n",
      "Jumlah baris sebelum drop: 18450\n",
      "Jumlah baris sesudah drop: 16526\n",
      "Jumlah baris yang dihapus: 1924\n",
      "Dataset cleaned berhasil disimpan ke E:\\$7th\\TA\\Eksploring_TF-IDF\\DATA\\Cleaned_DropNull.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = r'E:\\$7th\\TA\\Eksploring_TF-IDF\\DATA\\Cleaned_1.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"Dataset Loaded Successfully\")\n",
    "\n",
    "invalid_rows = df[\n",
    "    df['image_corelation'].isna() | (df['image_corelation'] == '-') |\n",
    "    df['Label'].isna() | (df['Label'] == '-')\n",
    "]\n",
    "\n",
    "print(\"Jumlah baris invalid unik:\", len(invalid_rows))\n",
    "# Buat mask untuk baris yang valid (bukan null dan bukan '-')\n",
    "mask = (\n",
    "    df['image_corelation'].notna() & (df['image_corelation'] != '-') &\n",
    "    df['Label'].notna() & (df['Label'] != '-')\n",
    ")\n",
    "\n",
    "# Terapkan mask → hanya ambil baris yang valid\n",
    "df_cleaned = df[mask].copy()\n",
    "\n",
    "print(\"Jumlah baris sebelum drop:\", len(df))\n",
    "print(\"Jumlah baris sesudah drop:\", len(df_cleaned))\n",
    "print(\"Jumlah baris yang dihapus:\", len(df) - len(df_cleaned))\n",
    "\n",
    "# Opsional: simpan ke file baru\n",
    "output_path = r'E:\\$7th\\TA\\Eksploring_TF-IDF\\DATA\\Cleaned_DropNull.csv'\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "print(f\"Dataset cleaned berhasil disimpan ke {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d91b6",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "ini gw mau coba cleaning dlu -> baru nanti di bobot pake TF IDF -> aku ambil kolom full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119c236",
   "metadata": {},
   "source": [
    "### PREPRO FULL_TEXT\n",
    "6 Step preprocessing dilakukan disini sesuai teori pada proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b728bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library yang dibutuhkan berhasil di-import\n",
      "Menggunakan Dataset: E:\\$7th\\TA\\Eksploring_TF-IDF\\DATA\\Cleaned_DropNull.csv\n",
      "Total rows: 16526\n",
      "Using text column: full_text\n",
      "\n",
      "Menyiapkan stemmer, custom stopword remover, dan kamus normalisasi...\n",
      "Berhasil memuat 675 kata dari kamus stopwords.\n",
      "Berhasil memuat 1018 kata dari kamus slang.\n",
      "\n",
      "Memulai proses preprocessing untuk setiap teks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Texts: 100%|██████████| 16526/16526 [1:09:24<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing selesai!\n",
      "Dokumen kosong setelah preprocessing: 2\n",
      "\n",
      "DataFrame final dengan kolom ['full_text', 'clean_text', 'image_corelation', 'Label'] berhasil disimpan di: E:\\$7th\\TA\\Eksploring_TF-IDF\\DATA\\Cleaned_Text.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== Preprocessing ===== #\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemover import StopWordRemover\n",
    "from Sastrawi.Dictionary.ArrayDictionary import ArrayDictionary\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Library yang dibutuhkan berhasil di-import\")\n",
    "\n",
    "#DATASET\n",
    "raw_path = r'E:\\$7th\\TA\\Eksploring_TF-IDF\\DATA\\Cleaned_DropNull.csv'  #input\n",
    "cleaned_text_path = r'E:\\$7th\\TA\\Eksploring_TF-IDF\\DATA\\Cleaned_Text.csv' #output\n",
    "\n",
    "#KAMUS\n",
    "stopwords_path = r'E:\\$7th\\TA\\Eksploring_TF-IDF\\Kamus\\combined_stop_words.txt'\n",
    "slang_path = r'E:\\$7th\\TA\\Eksploring_TF-IDF\\Kamus\\combined_slang_words.txt'\n",
    "exceptions_path = r'E:\\$7th\\TA\\Eksploring_TF-IDF\\Kamus\\combined_exceptions.txt'  \n",
    "\n",
    "# --- Memuat Dataset Utama ---\n",
    "df_full = pd.read_csv(raw_path)\n",
    "\n",
    "#df = df_full.head(100).copy() pake ngetes\n",
    "df = df_full.copy()\n",
    "print(f\"Menggunakan Dataset: {raw_path}\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Pilih kolom teks\n",
    "text_col = None\n",
    "for c in df.columns:\n",
    "    if 'full_text' in c.lower():\n",
    "        text_col = c\n",
    "        break\n",
    "if text_col is None:\n",
    "    raise RuntimeError(f\"Tidak ada kolom teks. Kolom tersedia: {df.columns.tolist()}\")\n",
    "print(f\"Using text column: {text_col}\")\n",
    "\n",
    "# --- INI SETUP TOOLS NYA ---\n",
    "print(\"\\nMenyiapkan stemmer, custom stopword remover, kamus normalisasi, dan exception list...\")\n",
    "\n",
    "# 1. Stemmer\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "# 2. Custom Stopword Remover -> ada di kamus stopwords (yang dianggap kata ga penting)\n",
    "try:\n",
    "    with open(stopwords_path, 'r') as f:\n",
    "        custom_stopword_list = [line.strip() for line in f]\n",
    "    dictionary = ArrayDictionary(custom_stopword_list)\n",
    "    stop_remover = StopWordRemover(dictionary)\n",
    "    print(f\"Berhasil memuat {len(custom_stopword_list)} kata dari kamus stopwords.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"PERINGATAN: File stopwords tidak ditemukan di '{stopwords_path}'.\")\n",
    "    stop_remover = StopWordRemover(ArrayDictionary([]))\n",
    "\n",
    "# 3. Kamus Normalisasi (slang → baku -> kalo ada lagi tambahin aja) \n",
    "try:\n",
    "    with open(slang_path, 'r') as f:\n",
    "        slang_dict = json.load(f)\n",
    "    print(f\"Berhasil memuat {len(slang_dict)} kata dari kamus slang.\")\n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "    print(f\"PERINGATAN: Gagal memuat kamus slang dari '{slang_path}'. Error: {e}\")\n",
    "    slang_dict = {}\n",
    "\n",
    "# 4. Exception List -> ada beberapa kata dari sastrawi yang di stemnya aneh jadi kalo nemu bisa tambah di exception list biar ga diproses\n",
    "try:\n",
    "    with open(exceptions_path, 'r') as f:\n",
    "        exception_list = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Berhasil memuat {len(exception_list)} kata dari exception list.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"PERINGATAN: File exception list tidak ditemukan di '{exceptions_path}'.\")\n",
    "    exception_list = []\n",
    "\n",
    "# --- LOOP STEP PREPROCESSING NYA DISINI---\n",
    "texts_to_process = df[text_col].fillna(\"\").astype(str).tolist()\n",
    "processed_texts = [] \n",
    "\n",
    "print(\"\\nMemulai proses preprocessing untuk setiap teks...\")\n",
    "for text in tqdm(texts_to_process, desc=\"Preprocessing Texts\"):\n",
    "    # 1. Cleaning\n",
    "         # Hapus URL\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "        # Hapus mention & hashtag\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "        # Ganti range angka \n",
    "    text = re.sub(r'(\\d+)\\s*-\\s*(\\d+)', r'\\1_\\2', text)\n",
    "\n",
    "        # Hapus kata yang cuma 1 huruf\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "        # Hilangkan tanda strip di akhir kata (contoh: prabowo- → prabowo)\n",
    "    text = re.sub(r'-\\b', '', text)\n",
    "\n",
    "        # Hapus karakter selain huruf, angka, spasi, underscore\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s_]', ' ', text)\n",
    "\n",
    "        # Rapikan spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 2. Case Folding Lowercase\n",
    "    text = text.lower()\n",
    "        \n",
    "    #3. Normalisasi slang\n",
    "    words = text.split()\n",
    "    normalized_words = [slang_dict.get(word, word) for word in words]\n",
    "    text = \" \".join(normalized_words)\n",
    "\n",
    "    #4. Hapus stopwords\n",
    "    text = stop_remover.remove(text)\n",
    "\n",
    "    # 5 & 6 Tokenization & Stemming dengan exception\n",
    "    stemmed_words = []\n",
    "    for w in text.split():\n",
    "        if w in exception_list:\n",
    "            stemmed_words.append(w)  # skip stemming untuk kata di exception list\n",
    "        else:\n",
    "            stemmed_words.append(stemmer.stem(w))\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    processed_texts.append(text)\n",
    "    \n",
    "# Masukkan hasil akhir ke DataFrame\n",
    "df['clean_text'] = processed_texts\n",
    "\n",
    "print(\"\\nPreprocessing selesai!\")\n",
    "\n",
    "#print(\"\\n===== HASIL PERBANDINGAN (100 TWEET) =====\\n\")\n",
    "#print(df[[text_col, 'clean_text']].to_string())\n",
    "\n",
    "print(\"Dokumen kosong setelah preprocessing:\", (df[\"clean_text\"].str.strip() == \"\").sum())\n",
    "\n",
    "# --- Simpan Hasil Akhir ke CSV ---\n",
    "columns_to_keep = [text_col, 'clean_text', 'image_corelation', 'Label']\n",
    "\n",
    "      # Buat DataFrame baru hanya dengan kolom-kolom tersebut\n",
    "final_df = df[columns_to_keep]\n",
    "\n",
    "      # Simpan DataFrame final ke CSV\n",
    "final_df.to_csv(cleaned_text_path, index=False)\n",
    "\n",
    "print(f\"\\nDataFrame final dengan kolom {columns_to_keep} berhasil disimpan di: {cleaned_text_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ded3d",
   "metadata": {},
   "source": [
    "### SAVE HASIL TF-IDF ke PKL dan NPZ\n",
    "NPZ ini bentuknya bobot-bobot TF-IDF yang udah di matrix jadi nanti kalo mau di fusion tinggal panggil file ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac199ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TfidfVectorizer...\n",
      "TF-IDF matrix shape: (16525, 20000)\n",
      "\n",
      "Top 25 (rare) features:\n",
      " 1mdb (idf=10.020)\n",
      " mesin ekonomi (idf=10.020)\n",
      " mbz (idf=10.020)\n",
      " to ganjel (idf=10.020)\n",
      " sgro (idf=10.020)\n",
      " ganjel (idf=10.020)\n",
      " 200 cat (idf=10.020)\n",
      " mie gaco (idf=10.020)\n",
      " taipei (idf=10.020)\n",
      " telur omega (idf=10.020)\n",
      " keramat (idf=10.020)\n",
      " venture capital (idf=10.020)\n",
      " keyk (idf=10.020)\n",
      " fn (idf=10.020)\n",
      " too high (idf=10.020)\n",
      " terusin (idf=10.020)\n",
      " crypto winter (idf=10.020)\n",
      " knight (idf=10.020)\n",
      " kut (idf=10.020)\n",
      " lembur (idf=10.020)\n",
      " kdslots (idf=10.020)\n",
      " wtb wtb (idf=10.020)\n",
      " tpid (idf=10.020)\n",
      " tus (idf=10.020)\n",
      " 1002 (idf=10.020)\n",
      "\n",
      "Saved vectorizer & matrix in: E:\\$7th\\TA\\Eksploring_TF-IDF\\DATA\\tfidf_artifacts\n"
     ]
    }
   ],
   "source": [
    "# ===== TF-IDF (fit & transform) =====\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "\n",
    "# Load dataframe hasil preprocessing\n",
    "cleaned_text_path = r'E:\\$7th\\TA\\Eksploring_TF-IDF\\DATA\\Cleaned_Text.csv'\n",
    "df = pd.read_csv(cleaned_text_path)\n",
    "texts = df[\"clean_text\"].fillna(\"\").astype(str)\n",
    "texts = texts[texts.str.strip() != \"\"]\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1,2),   # unigram + bigram\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
    ")\n",
    "\n",
    "print(\"Fitting TfidfVectorizer...\")\n",
    "X = vectorizer.fit_transform(texts)\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n",
    "\n",
    "# Diagnostics: top IDF terms\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "idf = vectorizer.idf_\n",
    "top_idx = np.argsort(idf)[::-1][:25]\n",
    "print(\"\\nTop 25 (rare) features:\")\n",
    "for i in top_idx:\n",
    "    print(f\" {feature_names[i]} (idf={idf[i]:.3f})\")\n",
    "\n",
    "# Save artifacts\n",
    "out_dir = os.path.join(os.path.dirname(cleaned_text_path), 'tfidf_artifacts')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(out_dir, \"tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "sparse.save_npz(os.path.join(out_dir, \"tfidf_matrix.npz\"), X)\n",
    "\n",
    "print(\"\\nSaved vectorizer & matrix in:\", out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
