{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41441a9",
   "metadata": {},
   "source": [
    "## CRAWL dari CNBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05067ebc",
   "metadata": {},
   "source": [
    "### Def untuk crawling dari Portal CNBC\n",
    "Note : \n",
    "- ganti User-Agent pada headers -> ke user agent browser sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15856fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def republika_batch(keywords, n_berita=200):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "    }\n",
    "\n",
    "    save_dir = r\"E:\\$7th\\TA\\Multimodal_Process_Exploration\\DATA\\corpus\\republika\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for keyword in keywords:\n",
    "        print(f\"\\nğŸš€ Mulai scraping: {keyword}\")\n",
    "        data_republika = []\n",
    "        page = 1\n",
    "\n",
    "        while len(data_republika) < n_berita:\n",
    "\n",
    "            alamatURL =  f'https://republika.co.id/search/v3/?q={keyword}&sortby=time&page={page}'\n",
    "            req = requests.get(alamatURL, headers=headers)\n",
    "            print(f\"ğŸ”„ Halaman {page} untuk {keyword}\")\n",
    "\n",
    "            if req.status_code != 200:\n",
    "                print(\"âŒ Gagal akses halaman.\")\n",
    "                break\n",
    "\n",
    "            # pakai parser fallback kalau lxml error\n",
    "            try:\n",
    "                soup = BeautifulSoup(req.content, 'lxml')\n",
    "            except Exception:\n",
    "                soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "            articles = soup.find_all('div', class_='articleItem')\n",
    "            if not articles:\n",
    "                print(\"âŒ Tidak ada artikel lagi.\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    a_tag = article.find('a', href=True)\n",
    "                    if not a_tag:\n",
    "                        continue\n",
    "\n",
    "                    url = a_tag['href']\n",
    "                    title_tag = article.find('h2', class_='articleTitle')\n",
    "                    judul = title_tag.get_text(strip=True) if title_tag else a_tag.get_text(strip=True)\n",
    "\n",
    "                    # buka halaman detail\n",
    "                    detail_req = requests.get(url, headers=headers)\n",
    "                    try:\n",
    "                        detail_soup = BeautifulSoup(detail_req.content, 'lxml')\n",
    "                    except Exception:\n",
    "                        detail_soup = BeautifulSoup(detail_req.content, 'html.parser')\n",
    "\n",
    "                    body = detail_soup.find('div', class_='read__content')\n",
    "                    berita = body.get_text(separator=' ', strip=True) if body else \"Tidak ditemukan\"\n",
    "\n",
    "                    if berita and len(berita) > 30:\n",
    "                        data_republika.append({\n",
    "                            'judul': judul,\n",
    "                            'berita': berita,\n",
    "                            'url': url,\n",
    "                        })\n",
    "                        print(f\"[{len(data_republika)}] {judul}\")\n",
    "\n",
    "                        if len(data_republika) >= n_berita:\n",
    "                            break\n",
    "\n",
    "                    time.sleep(1)  # delay biar ga ke-block\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"âš  Error ambil artikel:\", e)\n",
    "                    continue\n",
    "\n",
    "            page += 1\n",
    "\n",
    "        # Simpan hasil per keyword\n",
    "        if data_republika:\n",
    "            filename = os.path.join(save_dir, f\"kompas_news_{keyword.lower().replace(' ','')}_{time.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "            with open(filename, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=[\"judul\", \"berita\", \"url\"])\n",
    "                writer.writeheader()\n",
    "                writer.writerows(data_republika)\n",
    "\n",
    "            print(f\"\\nâœ… {len(data_republika)} artikel '{keyword}' disimpan ke {filename}\")\n",
    "        else:\n",
    "            print(f\"âŒ Tidak ada data valid untuk '{keyword}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212eba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Mulai scraping: BI rate\n",
      "ğŸ”„ Halaman 1 untuk BI rate\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'BI rate'\n",
      "\n",
      "ğŸš€ Mulai scraping: Pertumbuhan ekonomi\n",
      "ğŸ”„ Halaman 1 untuk Pertumbuhan ekonomi\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Pertumbuhan ekonomi'\n",
      "\n",
      "ğŸš€ Mulai scraping: Inflasi\n",
      "ğŸ”„ Halaman 1 untuk Inflasi\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Inflasi'\n",
      "\n",
      "ğŸš€ Mulai scraping: Bank Indonesia\n",
      "ğŸ”„ Halaman 1 untuk Bank Indonesia\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Bank Indonesia'\n",
      "\n",
      "ğŸš€ Mulai scraping: Perbankan digital\n",
      "ğŸ”„ Halaman 1 untuk Perbankan digital\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Perbankan digital'\n",
      "\n",
      "ğŸš€ Mulai scraping: IHSG\n",
      "ğŸ”„ Halaman 1 untuk IHSG\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'IHSG'\n",
      "\n",
      "ğŸš€ Mulai scraping: Pelemahan Rupiah\n",
      "ğŸ”„ Halaman 1 untuk Pelemahan Rupiah\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Pelemahan Rupiah'\n",
      "\n",
      "ğŸš€ Mulai scraping: Investasi\n",
      "ğŸ”„ Halaman 1 untuk Investasi\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Investasi'\n",
      "\n",
      "ğŸš€ Mulai scraping: Ekonomi Indonesia\n",
      "ğŸ”„ Halaman 1 untuk Ekonomi Indonesia\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Ekonomi Indonesia'\n",
      "\n",
      "ğŸš€ Mulai scraping: Danantara\n",
      "ğŸ”„ Halaman 1 untuk Danantara\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Danantara'\n",
      "\n",
      "ğŸš€ Mulai scraping: OJK\n",
      "ğŸ”„ Halaman 1 untuk OJK\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'OJK'\n",
      "\n",
      "ğŸš€ Mulai scraping: Reksadana\n",
      "ğŸ”„ Halaman 1 untuk Reksadana\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Reksadana'\n",
      "\n",
      "ğŸš€ Mulai scraping: Bursa Efek Indonesia\n",
      "ğŸ”„ Halaman 1 untuk Bursa Efek Indonesia\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Bursa Efek Indonesia'\n",
      "\n",
      "ğŸš€ Mulai scraping: BEI\n",
      "ğŸ”„ Halaman 1 untuk BEI\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'BEI'\n",
      "\n",
      "ğŸš€ Mulai scraping: Ekonomi digital\n",
      "ğŸ”„ Halaman 1 untuk Ekonomi digital\n",
      "âŒ Tidak ada artikel lagi.\n",
      "âŒ Tidak ada data valid untuk 'Ekonomi digital'\n"
     ]
    }
   ],
   "source": [
    "keywords = [\n",
    "    \"BI rate\",\n",
    "    \"Pertumbuhan ekonomi\",\n",
    "    \"Inflasi\",\n",
    "    \"Bank Indonesia\",\n",
    "    \"Perbankan digital\",\n",
    "    \"IHSG\",\n",
    "    \"Pelemahan Rupiah\",\n",
    "    \"Investasi\",\n",
    "    \"Ekonomi Indonesia\",\n",
    "    \"Danantara\",\n",
    "    \"OJK\",\n",
    "    \"Reksadana\",\n",
    "    \"Bursa Efek Indonesia\",\n",
    "    \"BEI\",\n",
    "    \"Ekonomi digital\"\n",
    "]\n",
    "\n",
    "republika_batch(keywords, n_berita=10000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
