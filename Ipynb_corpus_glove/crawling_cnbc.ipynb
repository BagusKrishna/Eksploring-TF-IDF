{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41441a9",
   "metadata": {},
   "source": [
    "## CRAWL dari CNBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05067ebc",
   "metadata": {},
   "source": [
    "### Def untuk crawling dari Portal CNBC\n",
    "Note : \n",
    "- ganti User-Agent pada headers -> ke user agent browser sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15856fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "headers_utama = {\n",
    "        #'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36 Edg/141.0.0.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "def cnbc_news_search(keyword, n_berita):\n",
    "    headers = headers_utama\n",
    "\n",
    "    data_cnbc = []\n",
    "    seen_urls = set()  # Untuk track URL yang sudah disimpan\n",
    "    page = 1\n",
    "\n",
    "    print(f\"Mulai scraping berita dari CNBC Indonesia dengan kata kunci '{keyword}'...\")\n",
    "\n",
    "    while len(data_cnbc) < n_berita:\n",
    "        encoded_keyword = urllib.parse.quote(keyword)\n",
    "        search_url = f'https://www.cnbcindonesia.com/search?query={encoded_keyword}&page={page}'\n",
    "        print(f\"\\nüîÑ Mengambil halaman {page} dari URL: {search_url}\")\n",
    "\n",
    "        try:\n",
    "            res = requests.get(search_url, headers=headers, timeout=20)\n",
    "            res.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Error saat mengakses halaman pencarian: {e}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        # Selector artikel untuk CNBC Indonesia\n",
    "        selectors = [\n",
    "            'div.list_search',\n",
    "            'div[class*=\"list\"]',\n",
    "            'div[class*=\"search\"]',\n",
    "            'article',\n",
    "            'div[class*=\"item\"]'\n",
    "        ]\n",
    "\n",
    "        articles = []\n",
    "        for selector in selectors:\n",
    "            articles = soup.select(selector)\n",
    "            if articles:\n",
    "                print(f\"‚úÖ Menemukan artikel dengan selector: {selector}\")\n",
    "                break\n",
    "\n",
    "        if not articles:\n",
    "            print(\"‚ùå Tidak ada artikel yang ditemukan di halaman ini. Coba periksa struktur website.\")\n",
    "            print(\"HTML snippet untuk debugging:\")\n",
    "            print(soup.prettify()[:500])\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            if len(data_cnbc) >= n_berita:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                title_elem = None\n",
    "                url_berita = None\n",
    "\n",
    "                # Selector untuk title dan URL CNBC Indonesia\n",
    "                title_selectors = [\n",
    "                    'h4 a',\n",
    "                    'h3 a',\n",
    "                    'h2 a',\n",
    "                    'a[href*=\"/news/\"]',\n",
    "                    'a[href*=\"cnbcindonesia.com\"]',\n",
    "                    'div.title a',\n",
    "                    '.media_list_link'\n",
    "                ]\n",
    "\n",
    "                for selector in title_selectors:\n",
    "                    element = article.select_one(selector)\n",
    "                    if element and element.get('href'):\n",
    "                        title_elem = element\n",
    "                        url_berita = element['href']\n",
    "                        break\n",
    "\n",
    "                if not title_elem or not url_berita:\n",
    "                    continue\n",
    "\n",
    "                # Pastikan URL lengkap\n",
    "                if url_berita.startswith('/'):\n",
    "                    url_berita = 'https://www.cnbcindonesia.com' + url_berita\n",
    "\n",
    "                judul = title_elem.get_text(strip=True)\n",
    "\n",
    "                # Skip artikel tanpa judul\n",
    "                if not judul:\n",
    "                    continue\n",
    "\n",
    "                # Cek duplikat URL\n",
    "                if url_berita in seen_urls:\n",
    "                    continue\n",
    "                seen_urls.add(url_berita)\n",
    "\n",
    "                # Ambil tanggal\n",
    "                tanggal = 'Tanggal tidak ditemukan'\n",
    "                date_selectors = [\n",
    "                    'span.date',\n",
    "                    'time',\n",
    "                    'div.date',\n",
    "                    'span[class*=\"date\"]',\n",
    "                    'div[class*=\"date\"]',\n",
    "                    '[class*=\"time\"]'\n",
    "                ]\n",
    "\n",
    "                for selector in date_selectors:\n",
    "                    date_elem = article.select_one(selector)\n",
    "                    if date_elem:\n",
    "                        tanggal = date_elem.get_text(strip=True)\n",
    "                        break\n",
    "\n",
    "                print(f\"[{len(data_cnbc) + 1}] Memproses berita: '{judul}'\")\n",
    "\n",
    "                # Ambil konten detail berita\n",
    "                try:\n",
    "                    detail_res = requests.get(url_berita, headers=headers, timeout=20)\n",
    "                    detail_res.raise_for_status()\n",
    "                    detail_soup = BeautifulSoup(detail_res.content, 'html.parser')\n",
    "\n",
    "                    # Selector untuk konten CNBC Indonesia - Lebih komprehensif\n",
    "                    content_selectors = [\n",
    "                        'div.detail_text',\n",
    "                        'div.content_detail',\n",
    "                        'div.post_content',\n",
    "                        'div.entry-content',\n",
    "                        'div.article-content',\n",
    "                        'div[class*=\"detail\"]',\n",
    "                        'div[class*=\"content\"]',\n",
    "                        'div[class*=\"text\"]',\n",
    "                        'div[class*=\"body\"]',\n",
    "                        'section[class*=\"content\"]',\n",
    "                        'article',\n",
    "                        '.detail_area',\n",
    "                        '.post-content',\n",
    "                        '.entry-content'\n",
    "                    ]\n",
    "\n",
    "                    berita = \"\"\n",
    "                    content_found = False\n",
    "\n",
    "                    for selector in content_selectors:\n",
    "                        content_div = detail_soup.select_one(selector)\n",
    "                        if content_div:\n",
    "                            print(f\"üîç Menggunakan selector: {selector}\")\n",
    "\n",
    "                            # Hapus elemen yang tidak diinginkan (ads, related articles, etc.)\n",
    "                            unwanted_selectors = [\n",
    "                                'script', 'style', 'noscript',\n",
    "                                '.ads', '.advertisement', '[class*=\"ads\"]', '[id*=\"ads\"]',\n",
    "                                '.related', '.recommend', '[class*=\"related\"]',\n",
    "                                '.social', '.share', '[class*=\"social\"]', '[class*=\"share\"]',\n",
    "                                '.comment', '[class*=\"comment\"]',\n",
    "                                '.navigation', '.nav', '[class*=\"nav\"]',\n",
    "                                '.footer', '[class*=\"footer\"]',\n",
    "                                '.sidebar', '[class*=\"sidebar\"]',\n",
    "                                '.widget', '[class*=\"widget\"]',\n",
    "                                'iframe', 'embed', 'object'\n",
    "                            ]\n",
    "\n",
    "                            for unwanted_selector in unwanted_selectors:\n",
    "                                unwanted_elements = content_div.select(unwanted_selector)\n",
    "                                for elem in unwanted_elements:\n",
    "                                    elem.decompose()\n",
    "\n",
    "                            # Coba berbagai cara ekstraksi konten\n",
    "                            paragraphs = content_div.find_all(['p', 'div'])\n",
    "                            if paragraphs:\n",
    "                                # Filter paragraf yang memiliki teks substansial\n",
    "                                valid_paragraphs = []\n",
    "                                for p in paragraphs:\n",
    "                                    text = p.get_text(strip=True)\n",
    "                                    if len(text) > 20 and not any(skip_word in text.lower() for skip_word in\n",
    "                                        ['baca juga', 'lihat juga', 'subscribe', 'follow', 'share', 'comment']):\n",
    "                                        valid_paragraphs.append(text)\n",
    "\n",
    "                                if valid_paragraphs:\n",
    "                                    berita = ' '.join(valid_paragraphs)\n",
    "                                    content_found = True\n",
    "\n",
    "                            # Jika tidak ada paragraf yang valid, ambil semua teks\n",
    "                            if not content_found:\n",
    "                                berita = content_div.get_text(separator=' ', strip=True)\n",
    "                                # Clean up extra whitespace\n",
    "                                berita = ' '.join(berita.split())\n",
    "\n",
    "                            print(f\"üìù Panjang konten: {len(berita)} karakter\")\n",
    "                            if len(berita) > 200:  # Threshold lebih tinggi\n",
    "                                content_found = True\n",
    "                                break\n",
    "\n",
    "                    # Fallback: ambil semua paragraf dari halaman\n",
    "                    if not content_found or len(berita) < 100:\n",
    "                        print(\"üîÑ Mencoba fallback method: mengambil semua paragraf dari halaman.\")\n",
    "\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"‚ö† Gagal mengambil konten dari URL: {url_berita}. Error: {e}\")\n",
    "                    berita = \"Error mengambil konten.\"\n",
    "\n",
    "                # Simpan data jika berita valid\n",
    "                if len(berita.strip()) > 20:\n",
    "                    data_cnbc.append({\n",
    "                        'judul': judul,\n",
    "                        'berita': berita,\n",
    "                        'url': url_berita,\n",
    "                        'tanggal': tanggal\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Error saat memproses artikel: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Jeda antar halaman\n",
    "        time.sleep(3)\n",
    "        page += 1\n",
    "\n",
    "        # Safety break untuk menghindari infinite loop\n",
    "        if page > 1000:\n",
    "            print(\"‚ö† Mencapai batas maksimal halaman (1000). Menghentikan scraping.\")\n",
    "            break\n",
    "\n",
    "    # Simpan hasil ke CSV\n",
    "    if data_cnbc:\n",
    "        \n",
    "        save_dir = r'E:\\$7th\\TA\\Multimodal_Process_Exploration\\DATA\\corpus'\n",
    "        os.makedirs(save_dir, exist_ok=True)  # bikin folder kalau belum ada\n",
    "        filename = os.path.join(save_dir, f'cnbc_news_search_{keyword.lower().replace(\" \", \"_\")}.csv')\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['judul', 'berita', 'url', 'tanggal'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data_cnbc)\n",
    "        print(f\"\\n‚úÖ {len(data_cnbc)} berita berhasil disimpan ke {filename}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Tidak ada data yang ditemukan. Kemungkinan:\")\n",
    "        print(\"1. Struktur website telah berubah\")\n",
    "        print(\"2. Ada pembatasan akses (rate limiting)\")\n",
    "        print(\"3. Kata kunci tidak menghasilkan hasil\")\n",
    "        print(\"4. Halaman pencarian kosong\")\n",
    "\n",
    "    return data_cnbc\n",
    "\n",
    "def cnbc_advanced_search(keyword, n_berita, start_page):\n",
    "    \"\"\"\n",
    "    Versi advanced dengan lebih banyak opsi kustomisasi\n",
    "    \"\"\"\n",
    "    headers = headers_utama\n",
    "\n",
    "    data_cnbc = []\n",
    "    seen_urls = set()\n",
    "    page = start_page\n",
    "    retry_count = 0\n",
    "    max_retries = 3\n",
    "\n",
    "    print(f\"Mulai scraping berita CNBC Indonesia (Advanced)\")\n",
    "    print(f\"Kata kunci: '{keyword}' | Target: {n_berita} berita | Mulai dari halaman: {start_page}\")\n",
    "\n",
    "    while len(data_cnbc) < n_berita:\n",
    "        encoded_keyword = urllib.parse.quote(keyword)\n",
    "        search_url = f'https://www.cnbcindonesia.com/search?query={encoded_keyword}&page={page}'\n",
    "\n",
    "        try:\n",
    "            print(f\"\\nüîÑ Halaman {page} - Target tersisa: {n_berita - len(data_cnbc)} berita\")\n",
    "            res = requests.get(search_url, headers=headers, timeout=25)\n",
    "            res.raise_for_status()\n",
    "            retry_count = 0  # Reset retry count on success\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retry_count += 1\n",
    "            if retry_count <= max_retries:\n",
    "                print(f\"‚ö† Retry {retry_count}/{max_retries} untuk halaman {page}: {e}\")\n",
    "                time.sleep(5 * retry_count)  # Exponential backoff\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"‚ùå Gagal mengakses halaman {page} setelah {max_retries} percobaan: {e}\")\n",
    "                break\n",
    "\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        # Multiple selectors untuk fleksibilitas\n",
    "        article_selectors = [\n",
    "            'div.list_search',\n",
    "            'div[class*=\"media_list\"]',\n",
    "            'div[class*=\"search_item\"]',\n",
    "            'div[class*=\"list\"]',\n",
    "            'article'\n",
    "        ]\n",
    "\n",
    "        articles = []\n",
    "        for selector in article_selectors:\n",
    "            articles = soup.select(selector)\n",
    "            if articles:\n",
    "                print(f\"‚úÖ Ditemukan {len(articles)} artikel dengan selector: {selector}\")\n",
    "                break\n",
    "\n",
    "        if not articles:\n",
    "            print(f\"‚ùå Halaman {page} kosong atau struktur berubah\")\n",
    "            if page == start_page:\n",
    "                print(\"Debug HTML snippet:\")\n",
    "                print(soup.prettify()[:800])\n",
    "            page += 1\n",
    "            continue\n",
    "\n",
    "        articles_processed = 0\n",
    "        for article in articles:\n",
    "            if len(data_cnbc) >= n_berita:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                # Extract title and URL\n",
    "                title_elem = None\n",
    "                url_berita = None\n",
    "\n",
    "                link_selectors = [\n",
    "                    'h4 a', 'h3 a', 'h2 a',\n",
    "                    'a[href*=\"/news/\"]',\n",
    "                    'a[href*=\"/market/\"]',\n",
    "                    'a[href*=\"/tech/\"]',\n",
    "                    '.media_list_link'\n",
    "                ]\n",
    "\n",
    "                for selector in link_selectors:\n",
    "                    element = article.select_one(selector)\n",
    "                    if element and element.get('href'):\n",
    "                        title_elem = element\n",
    "                        url_berita = element['href']\n",
    "                        break\n",
    "\n",
    "                if not title_elem or not url_berita:\n",
    "                    continue\n",
    "\n",
    "                if url_berita.startswith('/'):\n",
    "                    url_berita = 'https://www.cnbcindonesia.com' + url_berita\n",
    "\n",
    "                judul = title_elem.get_text(strip=True)\n",
    "                if not judul or url_berita in seen_urls:\n",
    "                    continue\n",
    "\n",
    "                seen_urls.add(url_berita)\n",
    "\n",
    "                print(f\"[{len(data_cnbc) + 1}] {judul[:60]}...\")\n",
    "\n",
    "                # Get article content\n",
    "                berita = get_article_content(url_berita, headers)\n",
    "\n",
    "                if len(berita.strip()) > 30:\n",
    "                    data_cnbc.append({\n",
    "                        'judul': judul,\n",
    "                        'berita': berita,\n",
    "                        'url': url_berita,\n",
    "                    })\n",
    "                    articles_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Error memproses artikel: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"üìù Halaman {page}: {articles_processed} artikel berhasil diproses\")\n",
    "\n",
    "        if articles_processed == 0:\n",
    "            print(f\"‚ö† Tidak ada artikel valid di halaman {page}\")\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(2)  # Reduced delay\n",
    "\n",
    "        if page > start_page + 100:  # Safety limit\n",
    "            print(\"‚ö† Mencapai batas halaman maksimal\")\n",
    "            break\n",
    "\n",
    "    # Save results\n",
    "    if data_cnbc:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        save_dir = r'E:\\$7th\\TA\\Multimodal_Process_Exploration\\DATA\\corpus'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = os.path.join(save_dir, f'cnbc_news_{keyword.lower().replace(\" \", \"\")}{timestamp}.csv')\n",
    "\n",
    "\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['judul', 'berita', 'url', 'tanggal'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data_cnbc)\n",
    "\n",
    "        print(f\"\\n‚úÖ Scraping selesai!\")\n",
    "        print(f\"üìÑ {len(data_cnbc)} berita disimpan ke: {filename}\")\n",
    "        print(f\"üîç Kata kunci: {keyword}\")\n",
    "        print(f\"üìä Halaman yang diproses: {start_page} - {page-1}\")\n",
    "\n",
    "    return data_cnbc\n",
    "\n",
    "def get_article_content(url, headers):\n",
    "    \"\"\"\n",
    "    Helper function untuk mengambil konten artikel\n",
    "    \"\"\"\n",
    "    try:\n",
    "        detail_res = requests.get(url, headers=headers, timeout=15)\n",
    "        detail_res.raise_for_status()\n",
    "        detail_soup = BeautifulSoup(detail_res.content, 'html.parser')\n",
    "\n",
    "        content_selectors = [\n",
    "            'div.detail_text',\n",
    "            'div.content_detail',\n",
    "            'div.post_content',\n",
    "            'div.entry-content',\n",
    "            'div.article-content',\n",
    "            'div[class*=\"detail\"]',\n",
    "            'div[class*=\"content\"]',\n",
    "            'div[class*=\"text\"]',\n",
    "            'div[class*=\"body\"]',\n",
    "            'section[class*=\"content\"]',\n",
    "            'article',\n",
    "            '.detail_area',\n",
    "            '.post-content',\n",
    "            '.entry-content'\n",
    "        ]\n",
    "\n",
    "        for selector in content_selectors:\n",
    "            content_div = detail_soup.select_one(selector)\n",
    "            if content_div:\n",
    "                # Clean unwanted elements lebih agresif\n",
    "                unwanted_selectors = [\n",
    "                    'script', 'style', 'noscript',\n",
    "                    '.ads', '.advertisement', '[class*=\"ads\"]', '[id*=\"ads\"]',\n",
    "                    '.related', '.recommend', '[class*=\"related\"]',\n",
    "                    '.social', '.share', '[class*=\"social\"]', '[class*=\"share\"]',\n",
    "                    '.comment', '[class*=\"comment\"]',\n",
    "                    '.navigation', '.nav', '[class*=\"nav\"]',\n",
    "                    'iframe', 'embed', 'object'\n",
    "                ]\n",
    "\n",
    "                for unwanted_selector in unwanted_selectors:\n",
    "                    for unwanted in content_div.select(unwanted_selector):\n",
    "                        unwanted.decompose()\n",
    "\n",
    "                # Extract paragraphs dengan filtering\n",
    "                paragraphs = content_div.find_all(['p', 'div'])\n",
    "                if paragraphs:\n",
    "                    valid_content = []\n",
    "                    for p in paragraphs:\n",
    "                        text = p.get_text(strip=True)\n",
    "                        # Filter konten yang relevan\n",
    "                        if (len(text) > 20 and\n",
    "                            not any(skip in text.lower() for skip in\n",
    "                                   ['baca juga', 'lihat juga', 'subscribe', 'follow', 'share'])):\n",
    "                            valid_content.append(text)\n",
    "\n",
    "                    if valid_content:\n",
    "                        content = ' '.join(valid_content)\n",
    "                    else:\n",
    "                        content = content_div.get_text(separator=' ', strip=True)\n",
    "                        content = ' '.join(content.split())  # Clean whitespace\n",
    "                else:\n",
    "                    content = content_div.get_text(separator=' ', strip=True)\n",
    "                    content = ' '.join(content.split())\n",
    "\n",
    "                if len(content) > 200:  # Threshold lebih tinggi\n",
    "                    return content\n",
    "\n",
    "        # Fallback method - ambil semua paragraf dari halaman\n",
    "        all_paragraphs = detail_soup.find_all('p')\n",
    "        if all_paragraphs:\n",
    "            fallback_content = []\n",
    "            for p in all_paragraphs:\n",
    "                text = p.get_text(strip=True)\n",
    "                if len(text) > 30:  # Paragraf substansial saja\n",
    "                    fallback_content.append(text)\n",
    "\n",
    "            if fallback_content:\n",
    "                return ' '.join(fallback_content)\n",
    "\n",
    "        return \"Konten tidak dapat diambil - struktur website mungkin berubah.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "if __name__ == \"_main_\":\n",
    "    # Contoh penggunaan\n",
    "    print(\"=== CNBC Indonesia News Scraper ===\\n\")\n",
    "\n",
    "    # Opsi 1: Basic scraping\n",
    "    # cnbc_news_search(keyword=\"IKN\", n_berita=50)\n",
    "\n",
    "    # Opsi 2: Advanced scraping dengan kontrol lebih banyak\n",
    "    cnbc_advanced_search(keyword=\"food estate\", n_berita=50, start_page=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212eba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "cnbc_news_search(keyword=\"ihsg\", n_berita=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316472ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai scraping berita CNBC Indonesia (Advanced)\n",
      "Kata kunci: 'ihsg' | Target: 1000 berita | Mulai dari halaman: 500\n",
      "\n",
      "üîÑ Halaman 500 - Target tersisa: 1000 berita\n",
      "‚úÖ Ditemukan 12 artikel dengan selector: div[class*=\"list\"]\n",
      "[1] VIDEOJika Suku Bunga Tinggi Berlangsung Lama, Rupiah Kian Vo...\n",
      "[2] IHSG 6.700, Asing Ramai-Ramai Borong Saham IniMarket1 tahun ...\n",
      "[3] IHSG Masih Merah, Asing Kurangi Porsi di Deretan Saham IniMa...\n",
      "[4] Investasi Masih Suram, 2 Sektor Ini PemicunyaMarket1 tahun y...\n",
      "[5] VIDEODana Asing Deras Tinggalkan RI, Rupiah Tertekan Sampai ...\n",
      "[6] VIDEOMarket Focus: IHSG Lesu Hingga Pengusaha Minta Evaluasi...\n",
      "[7] MARKET COMMENTARYIHSG Terkapar Lagi, 6 Saham Big Cap Ini Bia...\n",
      "[8] Asing Makin Gencar Tarik Dana Keluar RI, IHSG Kian TertekanM...\n",
      "üìù Halaman 500: 8 artikel berhasil diproses\n",
      "\n",
      "üîÑ Halaman 501 - Target tersisa: 992 berita\n",
      "‚úÖ Ditemukan 12 artikel dengan selector: div[class*=\"list\"]\n",
      "[9] Ini Hantu dari AS yang Bikin Pasar Keuangan RI GemetarMarket...\n",
      "[10] Rupiah Babak Belur, Alarm Rp 16.000 Menyala Hari Ini?Market1...\n",
      "[11] IHSG & Rupiah Kompak Terkapar, Ada Apa dengan RI Pak Jokowi?...\n",
      "[12] ini Bocoran Cari Cuan di Tahun Politik dan Era Bunga TinggiM...\n",
      "[13] Asing Jual Saham Rp 17,24 T! Ini yang Paling Banyak DilepasM...\n",
      "[14] Saham Ini Diborong Asing Saat IHSG Ambles 1,32%Market1 tahun...\n",
      "üìù Halaman 501: 6 artikel berhasil diproses\n",
      "\n",
      "üîÑ Halaman 502 - Target tersisa: 986 berita\n",
      "‚úÖ Ditemukan 12 artikel dengan selector: div[class*=\"list\"]\n",
      "[15] Jokowi Blak-Blakan Penyebab Asing Tarik Uang dari RINews1 ta...\n",
      "[16] IHSG Terkapar, Asing Ramai-Ramai Tinggalkan IndonesiaMarket1...\n",
      "[17] Terjun! Begini Pergerakan IHSG Dalam SepekanMarket1 tahun ya...\n",
      "[18] Deretan Saham Tercuan dan Terboncos Selama SepekanMarket1 ta...\n",
      "[19] VIDEORupiah & IHSG Melemah, Momentum Tepat Untuk Berinvestas...\n",
      "[20] VIDEOEkonomi AS Tumbuh Solid, The Fed Tetap Naikkan Suku Bun...\n",
      "[21] VIDEOSimak! Tips Memilih Investasi Saham Saat IHSG AnjlokNew...\n",
      "[22] VIDEOIHSG & Rupiah Rungkad, Paling Tepat Investasi Apa?News1...\n",
      "[23] VIDEOIHSG Ditutup di Zona Penguatan, Sektor Keuangan Rebound...\n",
      "üìù Halaman 502: 9 artikel berhasil diproses\n"
     ]
    }
   ],
   "source": [
    "# Advanced\n",
    "cnbc_advanced_search(keyword=\"ihsg\", n_berita=1000, start_page=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
